<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Naive Bayes Classifier</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>

    <h1 id="naive-bayes">Introduction to Naive Bayes Classifier</h1>
    <a href="machine_learning.html" class="back-link">Back to Machine Learning Contents</a>
    <p>The Naive Bayes classifier is a probabilistic machine learning model that’s used for classification tasks. It is based on Bayes' Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.</p>
    
    <h2>How Naive Bayes Works</h2>
    <p>Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables in a learning problem. Bayes’ theorem is stated as:</p>
    <pre><code>P(A|B) = (P(B|A) * P(A)) / P(B)</code></pre>
    <p>Where:</p>
    <ul>
        <li><code>P(A|B)</code> is the posterior probability of class <code>A</code> given predictor <code>B</code>.</li>
        <li><code>P(B|A)</code> is the likelihood which is the probability of predictor <code>B</code> given class <code>A</code>.</li>
        <li><code>P(A)</code> is the prior probability of class <code>A</code>.</li>
        <li><code>P(B)</code> is the prior probability of predictor <code>B</code>.</li>
    </ul>
    
    <h3>Types of Naive Bayes Classifiers</h3>
    <p>There are three main types of Naive Bayes classifiers:</p>
    <ul>
        <li><strong>Gaussian Naive Bayes:</strong> Assumes that the continuous values associated with each class are distributed according to a Gaussian distribution.</li>
        <li><strong>Multinomial Naive Bayes:</strong> Used for discrete counts. For example, it is used for text classification where the predictors are the frequency of the words present in the document.</li>
        <li><strong>Bernoulli Naive Bayes:</strong> Similar to the multinomial Naive Bayes but the predictors are Boolean variables. This is useful if the features are binary (i.e., they take on only two values).</li>
    </ul>
    
    <h2>Example in Python</h2>
    <p>Here is a simple example of how to perform Naive Bayes classification using Python and the <code>scikit-learn</code> library:</p>
    <pre><code>import numpy as np
from sklearn.naive_bayes import GaussianNB

# Example data
X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
y = np.array([0, 0, 1, 1])

# Create a Gaussian Naive Bayes classifier
clf = GaussianNB()
clf.fit(X, y)

# Make predictions
predictions = clf.predict(X)

# Output results
print("Predictions:", predictions)</code></pre>
    
    <h2>Conclusion</h2>
    <p>Naive Bayes classifiers are simple, yet powerful and fast. They are particularly well-suited for large datasets and real-time predictions. Despite their simplicity, they have been found to perform surprisingly well in many complex real-world applications.</p>
    
    <a href="machine_learning.html" class="back-link">Back to Machine Learning Contents</a>

</body>
</html>
